#Q5 - EFS
  회사는 사용자 업로드 문서를 Amazon EBS 볼륨에 저장하는 단일 Amazon EC2 인스턴스를
  사용하여 AWS 에서 웹 애플리케이션을 호스팅하고 있습니다. 더 나은 확장성과 가용성을
  위해 이 회사는 아키텍처를 복제하고 다른 가용 영역에 두 번째 EC2 인스턴스와 EBS
  볼륨을 생성하여 Application Load Balancer 뒤에 배치했습니다. 이 변경을 완료한 후
  사용자는 웹 사이트를 새로 고칠 때마다 문서의 일부 또는 다른 하위 집합을 볼 수 있지만
  모든 문서를 동시에 볼 수는 없다고 보고했습니다.
  솔루션 설계자는 사용자가 모든 문서를 한 번에 볼 수 있도록 무엇을 제안해야 합니까?
  A. 두 EBS 볼륨에 모든 문서가 포함되도록 데이터를 복사합니다.
  B. 문서가 있는 서버로 사용자를 안내하도록 Application Load Balancer 를 구성합니다.
  C. 두 EBS 볼륨의 데이터를 Amazon EFS 로 복사합니다. 새 문서를 Amazon EFS 에
  저장하도록 애플리케이션을 수정합니다.
  D. 두 서버 모두에 요청을 보내도록 Application Load Balancer 를 구성합니다. 올바른
  서버에서 각 문서를 반환합니다.

    내가 생각한 답 : C -> EBS는 특정한 인스턴스에 직접 연결되어 해당 인스턴스만 사용할 수 있다. 
    EFS를 사용해서 여러 대의 인스턴스가 동시에 접속해 사용할 수 있는 공유 저장소를 사용하는게 적합할 것이라고 생각했다.

    해설 : C -> EBS와 EFS의 가장 큰 차이점 중 하나는 EBS는 단일 AZ안에서만 접근이 가능한 저장소인
    반면, EFS 는 다중 AZ 안에서도 접근이 가능한 저장소라는 점입니다. 위 문제에서는 초기
    단일 AZ 에서 운영하던 EC2 및 EBS 를 복제한뒤 AZ 를 2 중화하여 멀티 EC2 및 EBS
    시스템으로 구성하였지만, 각 AZ 내에서 공유되지 않는 EBS 저장소를 별도로
    운영하였기때문에 고객들에게 일관성있는 데이터를 제공할 수 없었던 것으로 보입니다.
    이는 각 AZ 의 EC2 인스턴스가 동일한 저장소를 공유하도록 함으로써 해결할 수 있을 것
    같습니다. 초기 EBS 에 저장되어있던 데이터들을 일관성있게 보정하여 EFS 로 일회성
    마이그레이션을 수행한뒤 EC2 어플리케이션 서버 인스턴스가 EBS가 아닌 EFS에 데이터를
    저장하도록 변경하는 것이 바람직해보입니다.

    
#Q12 - ELB
  글로벌 회사는 ALB(Application Load Balancer) 뒤의 Amazon EC2 인스턴스에서 웹
  애플리케이션을 호스팅합니다. 웹 애플리케이션에는 정적 데이터와 동적 데이터가 있습니다.
  회사는 정적 데이터를 Amazon S3 버킷에 저장합니다. 회사는 정적 데이터 및 동적
  데이터의 성능을 개선하고 대기 시간을 줄이기를 원합니다. 회사는 Amazon Route 53 에
  등록된 자체 도메인 이름을 사용하고 있습니다.
  솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
  A. S3 버킷과 ALB 를 오리진으로 포함하는 Amazon CloudFront 배포를 생성합니다.
  CloudFront 배포로 트래픽을 라우팅하도록 Route 53 을 구성합니다.
  B. ALB 가 오리진인 Amazon CloudFront 배포를 생성합니다. S3 버킷을 엔드포인트로
  포함하는 AWS Global Accelerator 표준 액셀러레이터를 생성합니다. CloudFront 배포로
  트래픽을 라우팅하도록 Route 53 을 구성합니다.
  C. S3 버킷을 오리진으로 포함하는 Amazon CloudFront 배포를 생성합니다. ALB 및
  CloudFront 배포를 엔드포인트로 포함하는 AWS Global Accelerator 표준 액셀러레이터를
  생성합니다. 가속기 DNS 이름을 가리키는 사용자 지정 도메인 이름을 만듭니다. 사용자
  지정 도메인 이름을 웹 애플리케이션의 끝점으로 사용합니다.
  D. ALB 가 오리진인 Amazon CloudFront 배포를 생성합니다. S3 버킷을 엔드포인트로
  포함하는 AWS Global Accelerator 표준 액셀러레이터를 생성합니다. 두 개의 도메인 이름을
  만듭니다. 하나의 도메인 이름이 동적 콘텐츠의 CloudFront DNS 이름을 가리키도록 합니다.
  다른 도메인 이름이 정적 콘텐츠에 대한 가속기 DNS 이름을 가리키도록 합니다. 도메인
  이름을 웹 애플리케이션의 끝점으로 사용합니다.

  내가 생각한 답 : 
  해설 : 

#Q4 - VPC
  애플리케이션은 VPC 의 Amazon EC2 인스턴스에서 실행됩니다. 애플리케이션은 Amazon
  S3 버킷에 저장된 로그를 처리합니다. EC2 인스턴스는 인터넷 연결 없이 S3 버킷에
  액세스해야 합니다.
  Amazon S3 에 대한 프라이빗 네트워크 연결을 제공하는 솔루션은 무엇입니까?
  A. S3 버킷에 대한 게이트웨이 VPC 엔드포인트를 생성합니다.
  B. Amazon CloudWatch Logs 로 로그를 스트리밍합니다. 로그를 S3 버킷으로 내보냅니다.
  C. Amazon EC2 에 인스턴스 프로파일을 생성하여 S3 액세스를 허용합니다.
  D. S3 엔드포인트에 액세스하기 위한 프라이빗 링크가 있는 Amazon API Gateway API 를
  생성합니다.

  내가 생각한 답 : 
  해설 : 

#Q1 - S3
  회사는 여러 대륙에 걸쳐 도시의 온도, 습도 및 대기압에 대한 데이터를 수집합니다.
  회사가 매일 각 사이트에서 수집하는 데이터의 평균 볼륨은 500GB 입니다. 각 사이트에는
  고속 인터넷 연결이 있습니다.
  이 회사는 이러한 모든 글로벌 사이트의 데이터를 단일 Amazon S3 버킷에 최대한 빨리
  집계하려고 합니다. 솔루션은 운영 복잡성을 최소화해야 합니다.
  어떤 솔루션이 이러한 요구 사항을 충족합니까?
  A. 대상 S3 버킷에서 S3 Transfer Acceleration 을 켭니다. 멀티파트 업로드를 사용하여
  사이트 데이터를 대상 S3 버킷에 직접 업로드합니다.
  B. 각 사이트의 데이터를 가장 가까운 리전의 S3 버킷에 업로드합니다. S3 교차 리전
  복제를 사용하여 대상 S3 버킷에 객체를 복사합니다. 그런 다음 원본 S3 버킷에서
  데이터를 제거합니다.
  C. AWS Snowball Edge Storage Optimized 디바이스 작업을 매일 예약하여 각 사이트에서
  가장 가까운 리전으로 데이터를 전송합니다. S3 교차 리전 복제를 사용하여 대상 S3 버킷에
  객체를 복사합니다.
  D. 각 사이트의 데이터를 가장 가까운 리전의 Amazon EC2 인스턴스로 업로드합니다.
  Amazon Elastic Block Store(Amazon EBS) 볼륨에 데이터를 저장합니다. 정기적으로 EBS
  스냅샷을 만들어 대상 S3 버킷이 포함된 리전에 복사합니다. 해당 리전에서 EBS 볼륨을
  복원합니다.

  내가 생각한 답 : 
  해설 : 

#Q14 - RDS
  회사는 Application Load Balancer 뒤의 Amazon EC2 인스턴스에서 전자 상거래
  애플리케이션을 실행합니다. 인스턴스는 여러 가용 영역에 걸쳐 Amazon EC2 Auto Scaling
  그룹에서 실행됩니다. Auto Scaling 그룹은 CPU 사용률 메트릭을 기반으로 확장됩니다.
  전자 상거래 애플리케이션은 대규모 EC2 인스턴스에서 호스팅되는 MySQL 8.0
  데이터베이스에 트랜잭션 데이터를 저장합니다.
  애플리케이션 로드가 증가하면 데이터베이스의 성능이 빠르게 저하됩니다. 애플리케이션은
  쓰기 트랜잭션보다 더 많은 읽기 요청을 처리합니다. 이 회사는 고가용성을 유지하면서
  예측할 수 없는 읽기 워크로드의 수요를 충족하도록 데이터베이스를 자동으로 확장하는
  솔루션을 원합니다.
  어떤 솔루션이 이러한 요구 사항을 충족합니까?
  A. 리더 및 컴퓨팅 기능을 위해 단일 노드와 함께 Amazon Redshift 를 사용하십시오.
  B. 단일 AZ 배포와 함께 Amazon RDS 사용 다른 가용 영역에 리더 인스턴스를 추가하도록
  Amazon RDS 를 구성합니다.
  C. 다중 AZ 배포와 함께 Amazon Aurora 를 사용합니다. Aurora 복제본을 사용하여 Aurora
  Auto Scaling 을 구성합니다.
  D. EC2 스팟 인스턴스와 함께 Memcached 용 Amazon ElastiCache 를 사용합니다.

  내가 생각한 답 : 
  해설 : 

#Q18 - Lambda
  애플리케이션 개발 팀은 큰 이미지를 더 작은 압축 이미지로 변환하는 마이크로서비스를
  설계하고 있습니다. 사용자가 웹 인터페이스를 통해 이미지를 업로드하면 마이크로
  서비스는 이미지를 Amazon S3 버킷에 저장하고, AWS Lambda 함수로 이미지를 처리 및
  압축하고, 다른 S3 버킷에 압축된 형태로 이미지를 저장해야 합니다.
  솔루션 설계자는 내구성이 있는 상태 비저장 구성 요소를 사용하여 이미지를 자동으로
  처리하는 솔루션을 설계해야 합니다.
  이러한 요구 사항을 충족하는 작업 조합은 무엇입니까? (2 개를 선택하세요.)
  A. Amazon Simple Queue Service(Amazon SQS) 대기열을 생성합니다. 이미지가 S3 버킷에
  업로드될 때 SQS 대기열에 알림을 보내도록 S3 버킷을 구성합니다.
  B. Amazon Simple Queue Service(Amazon SQS) 대기열을 호출 소스로 사용하도록 Lambda
  함수를 구성합니다. SQS 메시지가 성공적으로 처리되면 대기열에서 메시지를 삭제합니다.
  C. 새 업로드에 대해 S3 버킷을 모니터링하도록 Lambda 함수를 구성합니다. 업로드된
  이미지가 감지되면 메모리의 텍스트 파일에 파일 이름을 쓰고 텍스트 파일을 사용하여
  처리된 이미지를 추적합니다.
  D. Amazon EC2 인스턴스를 시작하여 Amazon Simple Queue Service(Amazon SQS)
  대기열을 모니터링합니다. 항목이 대기열에 추가되면 EC2 인스턴스의 텍스트 파일에 파일
  이름을 기록하고 Lambda 함수를 호출합니다.
  E. Amazon EventBridge(Amazon CloudWatch Events) 이벤트를 구성하여 S3 버킷을
  모니터링합니다. 이미지가 업로드되면 추가 처리를 위해 애플리케이션 소유자의 이메일
  주소와 함께 Amazon ample Notification Service(Amazon SNS) 주제에 알림을 보냅니다.

  내가 생각한 답 : 
  해설 : 
